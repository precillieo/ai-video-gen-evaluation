import streamlit as st
import os
import tempfile
from PIL import Image
from model import ModelManager
from evaluation import Evaluator
from visualization import Visualizer
from config import HF_TOKEN

st.set_page_config(page_title="AI Video/Image Gen Evaluation", layout="wide")

# Sidebar: Metrics Documentation
with st.sidebar.expander("ðŸ“Š Metrics Guide", expanded=False):
    st.markdown("""
**CLIP Score**: Text-image similarity (higher is better, >25% good)

**BLIP Caption**: AI-generated caption for qualitative check (should match prompt)

**LPIPS Score**: Perceptual similarity (lower is better, <0.3 good)

**SSIM**: Structural similarity (higher is better, >0.7 good)

**PSNR**: Signal-to-noise ratio (higher is better, >30 good)

**Motion Magnitude**: Average movement (prompt-dependent)

**Motion Smoothness**: Consistency of motion (lower is better, <2 good)

**Temporal Consistency**: Motion correlation (higher is better, >0.5 good)

**Blur Score**: Image sharpness (higher is better, >30 good)
    """)

# Initialize managers
model_manager = ModelManager()
evaluator = Evaluator(model_manager)
visualizer = Visualizer()

# Main app layout
st.title("AI Video/Image Gen Evaluation")

# Sidebar for model selection
st.sidebar.header("Model Settings")
evaluation_type = st.sidebar.selectbox(
    "Select Generation Type",
    ["Text to Image", "Text to Video", "Image to Video"]
)

# Memory management
st.sidebar.info(model_manager.get_memory_info())

if st.sidebar.button("Clear Memory"):
    model_manager.unload_models()
    st.sidebar.success("Memory cleared!")

# Main content area
st.header(f"{evaluation_type} Generation")

if evaluation_type == "Text to Image":
    prompt = st.text_area("Enter your prompt:", "A photo of a cute cat sitting on a wooden chair")
    
    col1, col2 = st.columns(2)
    with col1:
        guidance_scale = st.slider("Guidance Scale", 1.0, 20.0, 7.5, 0.5)
    with col2:
        steps = st.slider("Inference Steps", 10, 100, 30, 5)
    
    t2i_models = ["Stable Diffusion 1.5", "Stable Diffusion 2.1"]
    selected_models = st.multiselect(
        "Select models to evaluate",
        t2i_models,
        default=t2i_models
    )
    
    if not selected_models:
        st.warning("Please select at least one model to evaluate")
    elif st.button("Generate and Evaluate"):
        results_cols = st.columns(len(selected_models))
        metrics_cols = st.columns(len(selected_models))
        
        results = {}
        successful_models = []
        
        for i, model_name in enumerate(selected_models):
            with results_cols[i]:
                try:
                    image = evaluator.text_to_image(model_name, prompt, guidance_scale, steps)
                    if image is not None:
                        # Get comprehensive evaluation metrics
                        prompt_alignment = evaluator.evaluate_prompt_alignment(prompt, image)
                        visual_quality = evaluator.compute_visual_quality(image)
                        
                        results[model_name] = {
                            "image": image,
                            "clip_score": prompt_alignment["clip_score"],
                            "blip_caption": prompt_alignment["blip_caption"],
                            "blur_score": visual_quality["blur_score"],
                            "color_consistency": visual_quality["color_consistency"]
                        }
                        successful_models.append(model_name)
                        
                        # Display results
                        st.image(image, caption=f"Generated by {model_name}")
                        
                        # Display metrics
                        with metrics_cols[i]:
                            st.metric("CLIP Score", f"{results[model_name]['clip_score']:.2f}%")
                            st.metric("Blur Score", f"{results[model_name]['blur_score']:.2f}")
                            st.metric("Color Consistency", f"{results[model_name]['color_consistency']:.2f}")
                            st.text_area("BLIP Caption", results[model_name]["blip_caption"], height=100)
                except Exception as e:
                    st.error(f"Error with {model_name}: {str(e)}")
        
        if successful_models:
            # Display comparison charts
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("Model Comparison - CLIP Score")
                chart_data = {model: results[model]["clip_score"] for model in successful_models}
                fig = visualizer.plot_comparison_chart(
                    chart_data,
                    "Text-to-Image Model Comparison",
                    "CLIP Score (%)"
                )
                st.pyplot(fig)
            
            with col2:
                st.subheader("Model Comparison - Visual Quality")
                chart_data = {
                    model: {
                        "Blur Score": results[model]["blur_score"],
                        "Color Consistency": results[model]["color_consistency"]
                    }
                    for model in successful_models
                }
                fig = visualizer.plot_comparison_chart(
                    chart_data,
                    "Visual Quality Metrics",
                    "Score"
                )
                st.pyplot(fig)
        
        model_manager.unload_models()

elif evaluation_type == "Text to Video":
    st.warning("This feature is under development. For now, you can upload videos and evaluate them.")
    
    prompt = st.text_area("Enter your prompt:", "A cat walking in a garden")
    uploaded_videos = st.file_uploader("Upload videos to evaluate", type=["mp4", "avi", "mov"], accept_multiple_files=True)
    
    if uploaded_videos and st.button("Evaluate Videos"):
        if len(uploaded_videos) > 3:
            st.warning("Please upload a maximum of 3 videos for comparison")
            uploaded_videos = uploaded_videos[:3]
        
        results = {}
        video_cols = st.columns(len(uploaded_videos))
        temp_files = []
        for i, video_file in enumerate(uploaded_videos):
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
            temp_file.write(video_file.read())
            temp_file.close()
            temp_files.append(temp_file.name)
            with video_cols[i]:
                st.video(temp_file.name)
                try:
                    frames = evaluator.extract_frames(temp_file.name)
                    prompt_alignment = evaluator.evaluate_prompt_alignment(prompt, frames[0])
                    motion_metrics = evaluator.compute_motion_metrics(frames)
                    visual_quality = evaluator.compute_visual_quality(frames[0])
                    results[video_file.name] = {
                        "frames": frames,
                        "clip_score": prompt_alignment["clip_score"],
                        "blip_caption": prompt_alignment["blip_caption"],
                        "motion_magnitude": motion_metrics["motion_magnitude"],
                        "motion_smoothness": motion_metrics["motion_smoothness"],
                        "temporal_consistency": motion_metrics["temporal_consistency"],
                        "blur_score": visual_quality["blur_score"],
                        "path": temp_file.name
                    }

                    st.metric("CLIP Score", f"{results[video_file.name]['clip_score']:.2f}%")
                    st.text_area("BLIP Score", results[video_file.name]["blip_caption"], height=70)

                    st.metric("Motion Magnitude.", f"{results[video_file.name]['motion_magnitude']:.2f}")
                    st.metric("Motion Smoothness", f"{results[video_file.name]['motion_smoothness']:.2f}")
                    st.metric("Temporal Consistency.", f"{results[video_file.name]['temporal_consistency']:.2f}")

                    st.metric("Blur Score", f"{results[video_file.name]['blur_score']:.2f}")
                except Exception as e:
                    st.error(f"Error processing video {video_file.name}: {str(e)}")
        
        if results:
            st.markdown("---")
            st.markdown("## Video Comparison Visualizations")
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("Video Comparison - CLIP Score")
                chart_data = {name: results[name]["clip_score"] for name in results.keys()}
                fig = visualizer.plot_comparison_chart(
                    chart_data,
                    "Text-to-Video Evaluation",
                    "CLIP Score (%)"
                )
                st.pyplot(fig)
            with col2:
                st.subheader("Video Comparison - Motion Quality")
                chart_data = {
                    name: {
                        "Motion Magnitude": results[name]["motion_magnitude"],
                        "Motion Smoothness": results[name]["motion_smoothness"],
                        "Temporal Consistency": results[name]["temporal_consistency"]
                    }
                    for name in results.keys()
                }
                fig = visualizer.plot_comparison_chart(
                    chart_data,
                    "Motion Quality Metrics",
                    "Score"
                )
                st.pyplot(fig)
            st.subheader("Video Comparison - Visual Quality")
            chart_data = {
                name: {
                    "Blur Score": results[name]["blur_score"]
                }
                for name in results.keys()
            }
            fig = visualizer.plot_comparison_chart(
                chart_data,
                "Visual Quality Metrics",
                "Score"
            )
            st.pyplot(fig)
        # Cleanup
        for temp_path in temp_files:
            os.unlink(temp_path)
        model_manager.unload_models()

elif evaluation_type == "Image to Video":
    st.warning("This feature is under development. For now, you can upload a reference image and evaluate videos against it.")
    
    reference_image = st.file_uploader("Upload reference image", type=["jpg", "jpeg", "png"])
    uploaded_videos = st.file_uploader("Upload videos to evaluate", type=["mp4", "avi", "mov"], accept_multiple_files=True)
    
    if reference_image and uploaded_videos and st.button("Evaluate Videos"):
        # Load and display reference image
        ref_img = Image.open(reference_image)
        st.image(ref_img, caption="Reference Image", width=300)
        
        if len(uploaded_videos) > 3:
            st.warning("Please upload a maximum of 3 videos for comparison")
            uploaded_videos = uploaded_videos[:3]
        
        video_cols = st.columns(len(uploaded_videos))
        results = {}
        
        for i, video_file in enumerate(uploaded_videos):
            with video_cols[i]:
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
                temp_file.write(video_file.read())
                temp_file.close()
                
                st.subheader(f"Video {i+1}: {video_file.name}")
                st.video(temp_file.name)
                
                try:
                    frames = evaluator.extract_frames(temp_file.name)
                    
                    # Get comprehensive evaluation metrics
                    similarity = evaluator.compute_clip_score(ref_img, frames)
                    motion_metrics = evaluator.compute_motion_metrics(frames)
                    visual_quality = evaluator.compute_visual_quality(frames[0], ref_img)
                    
                    results[video_file.name] = {
                        "frames": frames,
                        "clip_score": similarity,
                        "motion_magnitude": motion_metrics["motion_magnitude"],
                        "motion_smoothness": motion_metrics["motion_smoothness"],
                        "temporal_consistency": motion_metrics["temporal_consistency"],
                        "lpips_score": visual_quality["lpips_score"],
                        "ssim": visual_quality["ssim"],
                        "psnr": visual_quality["psnr"],
                        "path": temp_file.name
                    }
                    
                    # Display metrics
                    st.metric("Image-Video Similarity", f"{results[video_file.name]['clip_score']:.2f}%")
                    st.metric("LPIPS Score", f"{results[video_file.name]['lpips_score']:.2f}")
                    st.metric("SSIM", f"{results[video_file.name]['ssim']:.2f}")
                    st.metric("PSNR", f"{results[video_file.name]['psnr']:.2f}")
                    st.metric("Motion Magnitude", f"{results[video_file.name]['motion_magnitude']:.2f}")
                except Exception as e:
                    st.error(f"Error processing video {video_file.name}: {str(e)}")
        
        if results:
            # Display comparison charts
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("Video Comparison - Image Similarity")
                chart_data = {
                    name: {
                        "CLIP Score": results[name]["clip_score"],
                        "LPIPS Score": results[name]["lpips_score"],
                        "SSIM": results[name]["ssim"],
                        "PSNR": results[name]["psnr"]
                    }
                    for name in results.keys()
                }
                fig = visualizer.plot_comparison_chart(
                    chart_data,
                    "Image-Video Similarity Metrics",
                    "Score"
                )
                st.pyplot(fig)
            
            with col2:
                st.subheader("Video Comparison - Motion Quality")
                chart_data = {
                    name: {
                        "Motion Magnitude": results[name]["motion_magnitude"],
                        "Motion Smoothness": results[name]["motion_smoothness"],
                        "Temporal Consistency": results[name]["temporal_consistency"]
                    }
                    for name in results.keys()
                }
                fig = visualizer.plot_comparison_chart(
                    chart_data,
                    "Motion Quality Metrics",
                    "Score"
                )
                st.pyplot(fig)
        
        # Cleanup
        for video_name in results:
            os.unlink(results[video_name]["path"])
        model_manager.unload_models()

st.markdown("---")
st.markdown("AI Video/Image Gen Evaluation | Created with Streamlit")